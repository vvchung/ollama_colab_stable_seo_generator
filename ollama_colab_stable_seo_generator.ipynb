{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "# 專業 SEO 標題產生器\n",
        "\n",
        "<p>此 Colab 檔案演示了如何在 Google Colab 環境中，透過較穩定的 Python 背景執行方法來啟動 Ollama 伺服器，並結合 LangChain 實現一個「專業SEO標題產生器」AI，專門生成符合 SEO 規範的網頁標題。</p>\n",
        "\n",
        "<p><b>執行步驟：</b></p>\n",
        "<ul>\n",
        "    <li>請<b>務必依照儲存格的順序</b>，點擊每個儲存格左側的「播放」按鈕來執行。</li>\n",
        "    <li><b>儲存格 3</b> 會需要一些時間來下載 Ollama 並啟動伺服器，請耐心等候其顯示「Ollama Server has been started」。</li>\n",
        "    <li><b>儲存格 4</b> 會下載語言模型，第一次執行會花費較多時間。</li>\n",
        "</ul>\n",
        "\n",
        "<p><i>此筆記本中在 Colab 運行 Ollama 的穩定方法參考了 <a href='https://www.myapollo.com.tw/blog/google-colab-ollama/'>My APOLLO 的教學文章</a>。</i></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "install_langchain",
        "title": "1. 安裝 LangChain 套件"
      },
      "outputs": [],
      "source": [
        "# 首先，安裝 LangChain 相關的 Python 套件\n",
        "!pip install langchain langchain_community -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_ollama_binary",
        "title": "2. 下載 Ollama 執行檔"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# 使用 curl 下載 Ollama 的 Linux 執行檔\n",
        "curl -fsSL https://github.com/ollama/ollama/releases/download/v0.1.33/ollama-linux-amd64 -o /usr/bin/ollama\n",
        "# 檢查下載是否成功\n",
        "if [ $? -ne 0 ]; then\n",
        "    echo \"Error: Failed to download Ollama binary.\"\n",
        "    exit 1\n",
        "fi\n",
        "# 賦予執行權限\n",
        "chmod +x /usr/bin/ollama\n",
        "# 檢查權限賦予是否成功\n",
        "if [ $? -ne 0 ]; then\n",
        "    echo \"Error: Failed to set execute permissions for Ollama binary.\"\n",
        "    exit 1\n",
        "fi\n",
        "echo \"Ollama binary downloaded and permissions set.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama_background",
        "title": "3. (核心) 使用 Python 在背景啟動 Ollama 伺服器"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "import signal\n",
        "\n",
        "ollama_log_file = \"ollama_serve.log\"\n",
        "\n",
        "# 停止任何正在執行的 Ollama 伺服器程序\n",
        "def stop_ollama_serve():\n",
        "    try:\n",
        "        # 查找 ollama serve 程序 ID\n",
        "        # 使用 pgrep 搭配 -f 搜尋完整命令列\n",
        "        pid_output = subprocess.check_output([\"pgrep\", \"-f\", \"ollama serve\"]).decode().strip()\n",
        "        pids = pid_output.split('\\n') if pid_output else []\n",
        "\n",
        "        for pid in pids:\n",
        "            if pid:\n",
        "                print(f\"找到正在運行的 Ollama 程序，PID：{pid}。嘗試停止它。\")\n",
        "                try:\n",
        "                    # 發送終止信號\n",
        "                    os.kill(int(pid), signal.SIGTERM)\n",
        "                    # 等待一段時間讓其停止\n",
        "                    time.sleep(2)\n",
        "                    # 檢查是否仍在運行並在必要時強制終止\n",
        "                    try:\n",
        "                        os.kill(int(pid), 0) # 檢查程序是否存在\n",
        "                        print(f\"Ollama 程序 {pid} 未停止。正在強制終止。\")\n",
        "                        os.kill(int(pid), signal.SIGKILL)\n",
        "                    except OSError:\n",
        "                        print(f\"Ollama 程序 {pid} 已成功停止。\")\n",
        "                except ProcessLookupError:\n",
        "                    print(f\"未找到 PID 為 {pid} 的程序，可能已經停止。\")\n",
        "                except Exception as e:\n",
        "                    print(f\"與程序 {pid} 交互時發生錯誤：{e}\")\n",
        "\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"未找到正在運行的 Ollama serve 程序。\")\n",
        "    except Exception as e:\n",
        "        print(f\"查找 Ollama 程序時發生錯誤：{e}\")\n",
        "\n",
        "\n",
        "# 定義一個函數，使用 nohup 在背景啟動 Ollama 伺服器\n",
        "def run_ollama_serve_nohup():\n",
        "    # 使用 nohup 在背景運行 ollama serve 命令，\n",
        "    # 將輸出重定向到日誌檔案。\n",
        "    command = [\"nohup\", \"/usr/bin/ollama\", \"serve\", \">\", ollama_log_file, \"2>&1\", \"&\"]\n",
        "    # 使用 subprocess.run 搭配 shell=True 執行命令\n",
        "    # 這對於正確解釋重定向和背景符號是必要的\n",
        "    subprocess.run(\" \".join(command), shell=True, check=True)\n",
        "\n",
        "print(\"正在停止任何現有的 Ollama 伺服器程序...\")\n",
        "stop_ollama_serve()\n",
        "\n",
        "print(\"正在使用 nohup 在背景啟動 Ollama 伺服器...\")\n",
        "\n",
        "# 啟動 Ollama 伺服器\n",
        "try:\n",
        "    run_ollama_serve_nohup()\n",
        "    print(f\"Ollama 伺服器已啟動。正在檢查日誌檔案 '{ollama_log_file}' 的狀態...\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"啟動 Ollama 伺服器時發生錯誤：{e}\")\n",
        "    print(\"嘗試檢查是否已經在運行...\")\n",
        "\n",
        "# 給伺服器一些時間啟動，然後檢查日誌檔案或 API\n",
        "time.sleep(10) # 增加初始等待時間\n",
        "\n",
        "ollama_ready = False\n",
        "timeout = 180 # 增加超時時間至 3 分鐘\n",
        "start_time = time.time()\n",
        "\n",
        "while not ollama_ready and (time.time() - start_time) < timeout:\n",
        "    # 檢查日誌檔案是否有伺服器正在運行的標誌\n",
        "    if os.path.exists(ollama_log_file):\n",
        "        with open(ollama_log_file, 'r') as f:\n",
        "            log_content = f.read()\n",
        "            if \"Listening on\" in log_content or \"serving\" in log_content.lower(): # 尋找伺服器正在服務的標誌\n",
        "                 print(\"日誌檔案顯示 Ollama 伺服器正在運行。\")\n",
        "                 ollama_ready = True\n",
        "                 break\n",
        "    # 同時嘗試連接到 API\n",
        "    try:\n",
        "        response = requests.head(\"http://localhost:11434\", timeout=5) # 增加超時時間\n",
        "        if response.status_code == 200 or response.status_code == 404:\n",
        "            print(\"Ollama API 可訪問。\")\n",
        "            ollama_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        pass # 等待時預期的連接錯誤\n",
        "    except requests.exceptions.ReadTimeout:\n",
        "        print(\"檢查 API 時讀取超時，伺服器可能啟動緩慢。\")\n",
        "        pass # 讀取超時時繼續等待\n",
        "\n",
        "    print(\"正在等待 Ollama 伺服器準備就緒...\")\n",
        "    time.sleep(5) # 增加檢查間隔時間\n",
        "\n",
        "if ollama_ready:\n",
        "    print(\"Ollama 伺服器已啟動並可訪問。\")\n",
        "else:\n",
        "    print(\"錯誤：Ollama 伺服器在超時時間內未準備就緒。\")\n",
        "    if os.path.exists(ollama_log_file):\n",
        "         print(f\"{ollama_log_file} 的內容：\")\n",
        "         with open(ollama_log_file, 'r') as f:\n",
        "             print(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pull_model",
        "title": "4. 下載語言模型"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "import os\n",
        "\n",
        "ollama_log_file = \"ollama_serve.log\"\n",
        "\n",
        "# 在拉取模型之前等待 Ollama 伺服器準備就緒\n",
        "ollama_ready = False\n",
        "timeout = 180 # 將超時時間增加到 3 分鐘\n",
        "start_time = time.time()\n",
        "\n",
        "while not ollama_ready and (time.time() - start_time) < timeout:\n",
        "    # 檢查日誌檔案是否有伺服器正在運行或出現錯誤的標誌\n",
        "    if os.path.exists(ollama_log_file):\n",
        "        with open(ollama_log_file, 'r') as f:\n",
        "            log_content = f.read()\n",
        "            if \"Listening on\" in log_content or \"serving\" in log_content.lower():\n",
        "                 print(\"日誌檔案顯示 Ollama 伺服器正在運行。\")\n",
        "                 ollama_ready = True\n",
        "                 break\n",
        "            elif \"error\" in log_content.lower():\n",
        "                 print(f\"Ollama 日誌檔案中檢測到錯誤：{log_content}\")\n",
        "                 break # 如果在日誌中找到錯誤，則退出迴圈\n",
        "\n",
        "    # 同時嘗試連接到 API\n",
        "    try:\n",
        "        response = requests.head(\"http://localhost:11434\", timeout=5) # 增加超時時間\n",
        "        if response.status_code == 200 or response.status_code == 404:\n",
        "            print(\"Ollama API 可訪問。\")\n",
        "            ollama_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        pass # 等待時預期的連接錯誤\n",
        "    except requests.exceptions.ReadTimeout:\n",
        "        print(\"檢查 API 時讀取超時，伺服器可能啟動緩慢。\")\n",
        "        pass # 讀取超時時繼續等待\n",
        "\n",
        "\n",
        "    print(\"正在等待 Ollama 伺服器準備就緒...\")\n",
        "    time.sleep(5) # 增加檢查間隔時間\n",
        "\n",
        "if ollama_ready:\n",
        "    print(\"Ollama 伺服器已啟動且可訪問。正在拉取模型。\")\n",
        "    # 伺服器確認運行後，我們可以下載模型\n",
        "    # 這個儲存格第一次執行時將花費幾分鐘下載模型檔案\n",
        "    # 如果模型已存在，它會快速完成\n",
        "    !ollama pull llama3\n",
        "else:\n",
        "    print(\"錯誤：Ollama 伺服器在超時時間內未準備就緒。無法拉取模型。\")\n",
        "    if os.path.exists(ollama_log_file):\n",
        "         print(f\"{ollama_log_file} 的內容：\")\n",
        "         with open(ollama_log_file, 'r') as f:\n",
        "             print(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_langchain_chain",
        "title": "5. 設定「專業文案寫手」Prompt 並初始化 LangChain"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# 初始化 LangChain 的 Ollama LLM，連接到我們在背景運行的伺服器\n",
        "llm = Ollama(model=\"llama3\")\n",
        "\n",
        "# 創建詳細的 Prompt Template，賦予 AI 一個專業的 SEO 文案寫手 persona\n",
        "template = \"\"\"\n",
        "你是一位擁有超過10年經驗的「專業SEO標題產生者」，專長是為部落格文章、新聞稿和網站頁面撰寫引人注目且高效的標題。\n",
        "\n",
        "**你的任務是：**\n",
        "根據我提供的「核心關鍵字」，生成5個符合以下技術和創意要求的標題：\n",
        "\n",
        "**SEO 技術要求：**\n",
        "1.  **關鍵字置前：** 盡可能將「核心關鍵字」放在標題的開頭。\n",
        "2.  **長度適中：** 標題長度應介於 20 到 30 個中文字之間，以確保在搜尋結果頁面 (SERP) 上能完整顯示。\n",
        "3.  **包含數字或疑問詞：** 適度地使用數字（例如「5個方法」）或疑問詞（例如「如何」、「為什麼」）可以有效提升點擊率。\n",
        "4.  **避免關鍵字堆砌：** 標題需通順自然，不可惡意重複關鍵字。\n",
        "\n",
        "**文案創意要求：**\n",
        "1.  **吸引點擊 (High CTR)：** 標題需具備高度吸引力，能夠激發目標受眾的好奇心，讓他們想要點擊閱讀更多內容。\n",
        "2.  **傳達價值：** 清楚地告訴讀者文章能為他們帶來什麼好處或解決什麼問題。\n",
        "3.  **設定明確的目標受眾 (TA)：** 標題應能隱晦或明確地指向特定的讀者群體。\n",
        "\n",
        "**輸出格式：**\n",
        "請直接提供5個標題，每個標題一行，不要有任何額外的說明或編號。\n",
        "\n",
        "---\n",
        "\n",
        "**核心關鍵字：** {keyword}\n",
        "\n",
        "**生成的5個標題：**\n",
        "\"\"\"\n",
        "\n",
        "# 創建 LangChain PromptTemplate\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"keyword\"])\n",
        "\n",
        "# 建立 LLMChain\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "print(\"專業SEO標題產生器 LangChain Chain 已準備就緒！可以執行最後一個儲存格來生成標題。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_chain_final",
        "title": "6. 執行並生成 SEO 標題"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# 在繼續之前檢查 Ollama 伺服器是否正在運行\n",
        "ollama_ready = False\n",
        "timeout = 60 # 秒\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"正在檢查 Ollama 伺服器是否可訪問...\")\n",
        "while not ollama_ready and (time.time() - start_time) < timeout:\n",
        "    try:\n",
        "        response = requests.head(\"http://localhost:11434\", timeout=1)\n",
        "        if response.status_code == 200 or response.status_code == 404:\n",
        "            ollama_ready = True\n",
        "            print(\"Ollama 伺服器可訪問。\")\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        pass # 等待時預期的連接錯誤\n",
        "\n",
        "    print(\"正在等待 Ollama 伺服器...\")\n",
        "    time.sleep(5)\n",
        "\n",
        "if not ollama_ready:\n",
        "    print(\"錯誤：Ollama 伺服器在超時時間內未準備就緒。\")\n",
        "    print(\"請重新運行「在背景啟動 Ollama 伺服器」儲存格，並等待它確認伺服器已準備就緒。\")\n",
        "else:\n",
        "    # 現在，我們可以提供一個關鍵字給我們的「專業SEO標題產生器」AI 開始工作。\n",
        "    # 您可以修改下面的 input_keyword 來生成不同主題的標題\n",
        "    input_keyword = \"數位孿生\"\n",
        "\n",
        "    print(f\"正在為關鍵字「{input_keyword}」生成 SEO 標題...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # 運行 LLMChain 並傳入關鍵字\n",
        "    # 確保 llm_chain 已在先前的儲存格 (setup_langchain_chain) 中定義\n",
        "    if 'llm_chain' in locals():\n",
        "        response = llm_chain.invoke(input_keyword)\n",
        "\n",
        "        # 輸出最終結果\n",
        "        print(response['text'])\n",
        "    else:\n",
        "        print(\"錯誤：LangChain chain (llm_chain) 未定義。\")\n",
        "        print(\"請先運行「設置 LangChain Chain」儲存格。\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}